services:
  web:
    image: node:22.21.1-alpine
    #network_mode: bridge
    working_dir: /app
    command: sh -lc "npm ci && npm run dev"
    env_file: ./frontend/.env
    environment:
      - NODE_ENV=development
      - WATCHPACK_POLLING=true
      - NEXT_TELEMETRY_DISABLED=1
    ports: ["3000:3000"]
    volumes:
      - ./frontend:/app
      - /app/node_modules
    depends_on: [api]

  api:
    image: node:22.21.1-alpine
    #network_mode: bridge
    working_dir: /app
    command: sh -lc "npm ci && npm run dev"
    env_file: ./backend/.env
    environment: [ NODE_ENV=development ]
    ports: ["4000:4000"]
    volumes:
      - ./backend:/app
      - /app/node_modules
    depends_on: [mongo, redis, llm]

  mongo:
    image: mongo:7.0.14
    volumes: [ "mongo-data:/data/db" ]

  redis:
    image: redis:7.2.4-alpine

  llm:
    image: ghcr.io/ggml-org/llama.cpp:full
    container_name: vn_llm
    entrypoint: ["/app/llama-server"]
    volumes:
      - ./models:/models:ro
    ports:
      - "8082:8080"
    command:
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8080"
      - "-m"
      - "/models/qwen3-0.6b-q4_k_m.gguf"
      - "-c"
      - "1024"
      - "-t"
      - "2"
      - "-tb"
      - "2"
      - "--threads-http"
      - "1"
      - "-np"
      - "1"
      - "--cache-ram"
      - "1024"
      - "--n-gpu-layers"
      - "0"
    restart: unless-stopped


volumes:
  mongo-data:
